{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Advance AI - Lab 1**\n",
        "\n",
        "<br>\n",
        "\n",
        "### **TOPIC - FEATURE ENGINEERING ON TEXT DATA**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h436KgCh3EpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####------------------------------------------------------------------\n",
        "\n",
        "####**Name:** Sabyasachi Singh\n",
        "\n",
        "<br>\n",
        "\n",
        "####**PRN:** 20190802051\n",
        "\n",
        "#----------------------------------------------"
      ],
      "metadata": {
        "id": "rrHcmzU8UFig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Github Link: https://github.com/killersabya/AiMl1/tree/main/lab_1"
      ],
      "metadata": {
        "id": "HU2Jrn7dk6a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Aim:** To perform segmentation and Feature Engineering on Text data."
      ],
      "metadata": {
        "id": "w71dXSuXSqIw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZgXMulZ_UGA"
      },
      "source": [
        "#### **To Extract:** Abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ------------------------------------------------------------------------------\n",
        "\n",
        "# >> Segmentation \n",
        "<br>\n",
        "\n",
        "#### **1) Installing necessary libraries**"
      ],
      "metadata": {
        "id": "7ojgXOuo3t8K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb--GXr-_TVG",
        "outputId": "53b0ac20-7851-4d34-800d-fd85d356b1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "# For Converting the PDF File and Extracting the Contents of the PDF File.\n",
        "!pip install PyPDF2\n",
        "\n",
        "# For Working with Text Data (NLP) like Removing the Punctuations, Stop Words, etc from the data.\n",
        "!pip install nltk\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2 as pdf\n",
        "\n",
        "# reading all the 3 pdf files\n",
        "pdf1 = pdf.PdfFileReader(\"/content/1.pdf\")\n",
        "pdf2 = pdf.PdfFileReader(\"/content/2.pdf\")\n",
        "pdf3 = pdf.PdfFileReader(\"/content/3.pdf\")\n"
      ],
      "metadata": {
        "id": "ma1aRSQuWejE"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the 3 pdf files' data into text variables.\n",
        "text1 = \"\"\n",
        "text2 = \"\"\n",
        "text3 = \"\"\n",
        "\n",
        "# we will be extracting Abstract from pdf files, which can be usually found on the first page.\n",
        "text1 = pdf1.getPage(0).extractText()\n",
        "text2 = pdf2.getPage(0).extractText()\n",
        "text3 = pdf3.getPage(0).extractText()"
      ],
      "metadata": {
        "id": "3ZAWq51-8ZwX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOcDCTEF1P52",
        "outputId": "71f8cf40-b6c0-458c-dea3-e85ac802c0df"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0162-8828 (c) 2016 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\n",
            "http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2644615, IEEE\n",
            "Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "SegNet: A Deep Convolutional\n",
            "Encoder-Decoder Architecture for Scene\n",
            "Segmentation\n",
            "Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, Senior Member, IEEE,\n",
            "Abstract—We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation\n",
            "termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed\n",
            "by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the\n",
            "VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature\n",
            "maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input\n",
            "feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to\n",
            "perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then\n",
            "convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2]\n",
            "and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus\n",
            "accuracy trade-off involved in achieving good segmentation performance.\n",
            "SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and\n",
            "computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing\n",
            "architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet\n",
            "and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments\n",
            "show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared\n",
            "to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.\n",
            "Index Terms—Deep Convolutional Neural Networks, Semantic Pixel-Wise Segmentation, Indoor Scenes, Road Scenes, Encoder,\n",
            "Decoder, Pooling, Upsampling.\n",
            "F\n",
            "1 I NTRODUCTION\n",
            "Semantic segmentation has a wide array of applications ranging\n",
            "from scene understanding, inferring support-relationships among\n",
            "objects to autonomous driving. Early methods that relied on low-\n",
            "level vision cues have fast been superseded by popular machine\n",
            "learning algorithms. In particular, deep learning has seen huge suc-\n",
            "cess lately in handwritten digit recognition, speech, categorising\n",
            "whole images and detecting objects in images [5], [6]. Now there\n",
            "is an active interest for semantic pixel-wise labelling [7] [8], [9],\n",
            "[2], [4], [10], [11], [12], [13], [3], [14], [15], [16]. However, some\n",
            "of these recent approaches have tried to directly adopt deep archi-\n",
            "tectures designed for category prediction to pixel-wise labelling\n",
            "[7]. The results, although very encouraging, appear coarse [3].\n",
            "This is primarily because max pooling and sub-sampling reduce\n",
            "feature map resolution. Our motivation to design SegNet arises\n",
            "from this need to map low resolution features to input resolution\n",
            "for pixel-wise classiﬁcation. This mapping must produce features\n",
            "which are useful for accurate boundary localization.\n",
            "Our architecture, SegNet, is designed to be an efﬁcient ar-\n",
            "chitecture for pixel-wise semantic segmentation. It is primarily\n",
            "motivated by road scene understanding applications which require\n",
            "the ability to model appearance (road, building), shape (cars,\n",
            "\u000fV . Badrinarayanan, A. Kendall, R. Cipolla are with the Machine Intelli-\n",
            "gence Lab, Department of Engineering, University of Cambridge, UK.\n",
            "E-mail: vb292,agk34,cipolla@eng.cam.ac.ukpedestrians) and understand the spatial-relationship (context) be-\n",
            "tween different classes such as road and side-walk. In typical road\n",
            "scenes, the majority of the pixels belong to large classes such\n",
            "as road, building and hence the network must produce smooth\n",
            "segmentations. The engine must also have the ability to delineate\n",
            "objects based on their shape despite their small size. Hence it is\n",
            "important to retain boundary information in the extracted image\n",
            "representation. From a computational perspective, it is necessary\n",
            "for the network to be efﬁcient in terms of both memory and\n",
            "computation time during inference. The ability to train end-to-end\n",
            "in order to jointly optimise all the weights in the network using\n",
            "an efﬁcient weight update technique such as stochastic gradient\n",
            "descent (SGD) [17] is an additional beneﬁt since it is more easily\n",
            "repeatable. The design of SegNet arose from a need to match these\n",
            "criteria.\n",
            "The encoder network in SegNet is topologically identical to\n",
            "the convolutional layers in VGG16 [1]. We remove the fully\n",
            "connected layers of VGG16 which makes the SegNet encoder\n",
            "network signiﬁcantly smaller and easier to train than many other\n",
            "recent architectures [2], [4], [11], [18]. The key component of\n",
            "SegNet is the decoder network which consists of a hierarchy\n",
            "of decoders one corresponding to each encoder. Of these, the\n",
            "appropriate decoders use the max-pooling indices received from\n",
            "the corresponding encoder to perform non-linear upsampling of\n",
            "their input feature maps. This idea was inspired from an archi-\n",
            "tecture designed for unsupervised feature learning [19]. Reusing\n",
            "max-pooling indices in the decoding process has several practical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting Segments i.e Title and Journal Name**"
      ],
      "metadata": {
        "id": "FQ7nm26oCsyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the text data into list, to access specific key elements\n",
        "\n",
        "pdfdatalist1 = text1.splitlines()\n",
        "\n",
        "pdfdatalist2 = text2.splitlines()\n",
        "\n",
        "pdfdatalist3 = text3.splitlines()"
      ],
      "metadata": {
        "id": "C9T7rq91_kua"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting titles from all pdf files**"
      ],
      "metadata": {
        "id": "f4WpAlUMKk7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdfdatalist1[8:24]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wm2zt92Al9-",
        "outputId": "f912d097-7953-40cc-f542-54c70cec40e3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Abstract—We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation',\n",
              " 'termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed',\n",
              " 'by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the',\n",
              " 'VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature',\n",
              " 'maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input',\n",
              " 'feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to',\n",
              " 'perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then',\n",
              " 'convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2]',\n",
              " 'and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus',\n",
              " 'accuracy trade-off involved in achieving good segmentation performance.',\n",
              " 'SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and',\n",
              " 'computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing',\n",
              " 'architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet',\n",
              " 'and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments',\n",
              " 'show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared',\n",
              " 'to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdfdatalist2[7:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbAhuAtzK_EN",
        "outputId": "e012493b-35c5-4bb9-e122-1e5ad320754c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Abstract—Image segmentation is a key task in computer vision and image processing with important applications such as scene',\n",
              " 'understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others,',\n",
              " 'and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of Deep Learning (DL) has',\n",
              " 'prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this',\n",
              " 'recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling',\n",
              " 'networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and',\n",
              " 'generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation',\n",
              " 'models, examine the widely used datasets, compare performances, and discuss promising research directions.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdfdatalist3[7:19]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-4vjZhwLGnN",
        "outputId": "1ed63d08-5d07-48ac-ed9e-3c683b262754"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.',\n",
              " 'Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region',\n",
              " 'proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image',\n",
              " 'convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional',\n",
              " 'network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to',\n",
              " 'generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN',\n",
              " 'into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with',\n",
              " '’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],',\n",
              " 'our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection',\n",
              " 'accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO',\n",
              " '2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been',\n",
              " 'made publicly available.']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Joining the titles and converting them into a single string\n",
        "\n",
        "d1 = pdfdatalist1[8:24]\n",
        "title_join_1 = \" \".join(d1)\n",
        "\n",
        "d2 = pdfdatalist2[7:15]\n",
        "title_join_2 = \" \".join(d2)\n",
        "\n",
        "d3 = pdfdatalist3[7:19]\n",
        "title_join_3 = \" \".join(d3)\n"
      ],
      "metadata": {
        "id": "r03LKUoULLYZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_join_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "LYIK_ljQy_pF",
        "outputId": "1b5a6642-ed01-4c6c-aca8-3ec31070c075"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Abstract—We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_join_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "Cda45oBJzEEU",
        "outputId": "ed84e544-2059-441b-b4d5-8206d65c1a35"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Abstract—Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of Deep Learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_join_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "jt-Noo0PzFpM",
        "outputId": "14fbee15-dfca-4b60-d522-2a74b31f997c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# >> Data Preprocessing"
      ],
      "metadata": {
        "id": "UD9mYXtODGSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = title_join_1.lower()\n",
        "\n",
        "data2 = title_join_2.lower()\n",
        "\n",
        "data3 = title_join_3.lower()"
      ],
      "metadata": {
        "id": "p-UIucFuNU21"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 "
      ],
      "metadata": {
        "id": "Pj_phx1sNg6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e1fb96e2-e0de-4436-a613-b1e8e30d056f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abstract—we present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed segnet. this core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classiﬁcation layer. the architecture of the encoder network is topologically identical to the 13 convolutional layers in the vgg16 network [1]. the role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classiﬁcation. the novelty of segnet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. this eliminates the need for learning to upsample. the upsampled maps are sparse and are then convolved with trainable ﬁlters to produce dense feature maps. we compare our proposed architecture with the widely adopted fcn [2] and also with the well known deeplab-largefov [3], deconvnet [4] architectures. this comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. segnet was primarily motivated by scene understanding applications. hence, it is designed to be efﬁcient both in terms of memory and computational time during inference. it is also signiﬁcantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. we also performed a controlled benchmark of segnet and other architectures on both road scenes and sun rgb-d indoor scene segmentation tasks. these quantitative assessments show that segnet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared to other architectures. we also provide a caffe implementation of segnet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "m8-R4cxgVw-W",
        "outputId": "b25e47e4-c172-4247-8506-f358911ce686"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abstract—image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. against this backdrop, the broad success of deep learning (dl) has prompted the development of new image segmentation approaches leveraging dl models. we provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. we investigate the relationships, strengths, and challenges of these dl-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "90pss3n_VzSF",
        "outputId": "171335db-2c8a-4542-9c29-dfc9232b1ea5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abstract—state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. advances like sppnet [1] and fast r-cnn [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to generate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the rpn component tells the uniﬁed network where to look. for the very deep vgg-16 model [3], our detection system has a frame rate of 5fps (including all steps ) on a gpu, while achieving state-of-the-art object detection accuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco 2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been made publicly available.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Removing Stop words and punctuation**"
      ],
      "metadata": {
        "id": "tZfb2paGZYvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing nltk and downloading stopwords\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "\n",
        "# contains list of all stop words in english language\n",
        "sw.words(\"english\")"
      ],
      "metadata": {
        "id": "eFu2gxK71PAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139c3493-6373-4b3d-bf47-20a6c6c6cde6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"for\" and \"with\", which are the stop words are actually not removed from the text\n",
        "# so to remove it completely, we need to clean the data and preprocess it.\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Loading english language model and creating nlp object from it.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def pre_process_data(text):\n",
        "  document = nlp(text)\n",
        "\n",
        "  filtered_words = []\n",
        "\n",
        "  for word in document:\n",
        "    if word.is_stop or word.is_punct:\n",
        "      continue\n",
        "\n",
        "    \n",
        "    filtered_words.append(word.lemma_)\n",
        "\n",
        "\n",
        "  return \" \".join(filtered_words)"
      ],
      "metadata": {
        "id": "ydO_qDHWe_wH"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pre_process_data(data1)\n",
        "data2 = pre_process_data(data2)\n",
        "data3 = pre_process_data(data3)"
      ],
      "metadata": {
        "id": "agXi2acdfr7T"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data 1 after pre processing:\\n\")\n",
        "print(data1)"
      ],
      "metadata": {
        "id": "vfKCitUsf0Ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a3cb6e-4e9a-4797-f1dc-b6783d2e732c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data 1 after pre processing:\n",
            "\n",
            "abstract present novel practical deep fully convolutional neural network architecture semantic pixel wise segmentation term segnet core trainable segmentation engine consist encoder network correspond decoder network follow pixel wise classiﬁcation layer architecture encoder network topologically identical 13 convolutional layer vgg16 network 1 role decoder network map low resolution encoder feature map input resolution feature map pixel wise classiﬁcation novelty segnet lie manner decoder upsample low resolution input feature map(s speciﬁcally decoder use pool index compute max pool step correspond encoder perform non linear upsampling eliminate need learn upsample upsampled map sparse convolve trainable ﬁlter produce dense feature map compare propose architecture widely adopt fcn 2 know deeplab largefov 3 deconvnet 4 architecture comparison reveal memory versus accuracy trade involve achieve good segmentation performance segnet primarily motivate scene understanding application design efﬁcient term memory computational time inference signiﬁcantly small number trainable parameter compete architecture train end end stochastic gradient descent perform control benchmark segnet architecture road scene sun rgb d indoor scene segmentation task quantitative assessment segnet provide good performance competitive inference time efﬁcient inference memory wise compare architecture provide caffe implementation segnet web demo http://mi.eng.cam.ac.uk/projects/segnet/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data 2 after pre processing:\\n\")\n",
        "print(data2)"
      ],
      "metadata": {
        "id": "IYBhqzOff1ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b205694-603b-4595-b80f-63b175e5a1a1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data 2 after pre processing:\n",
            "\n",
            "abstract image segmentation key task computer vision image processing important application scene understanding medical image analysis robotic perception video surveillance augmented reality image compression numerous segmentation algorithm find literature backdrop broad success deep learning dl prompt development new image segmentation approach leverage dl model provide comprehensive review recent literature cover spectrum pioneering effort semantic instance segmentation include convolutional pixel labeling network encoder decoder architecture multiscale pyramid base approach recurrent network visual attention model generative model adversarial setting investigate relationship strength challenge dl base segmentation model examine widely dataset compare performance discuss promise research direction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data 3 after pre processing:\\n\")\n",
        "print(data3)"
      ],
      "metadata": {
        "id": "xPd9X4fcf4wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a8b639-a0bc-42f8-8606-bfa3eca749d3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data 3 after pre processing:\n",
            "\n",
            "abstract state art object detection network depend region proposal algorithm hypothesize object location advance like sppnet 1 fast r cnn 2 reduce run time detection network expose region proposal computation bottleneck work introduce region proposal network rpn share image convolutional feature detection network enable nearly cost free region proposal rpn fully convolutional network simultaneously predict object bound objectness score position rpn train end end generate high quality region proposal fast r cnn detection merge rpn fast r cnn single network share convolutional feature recently popular terminology neural network attention mechanism rpn component tell uniﬁed network look deep vgg-16 model 3 detection system frame rate 5fps include step gpu achieve state art object detection accuracy pascal voc 2007 2012 ms coco dataset 300 proposal image ilsvrc coco 2015 competition fast r cnn rpn foundation 1st place win entry track code publicly available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# >> Feature Engineering"
      ],
      "metadata": {
        "id": "qJM-g_rXWY_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Converting string data into vector format so that we can feed it into the model.**"
      ],
      "metadata": {
        "id": "FFU3DeUvW4YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1_list_format = data1.split()\n",
        "\n",
        "print(data1_list_format)"
      ],
      "metadata": {
        "id": "sw3ch_k_aGUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2eb32ad-479e-43a1-909a-dfea2e798b8c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abstract', 'present', 'novel', 'practical', 'deep', 'fully', 'convolutional', 'neural', 'network', 'architecture', 'semantic', 'pixel', 'wise', 'segmentation', 'term', 'segnet', 'core', 'trainable', 'segmentation', 'engine', 'consist', 'encoder', 'network', 'correspond', 'decoder', 'network', 'follow', 'pixel', 'wise', 'classiﬁcation', 'layer', 'architecture', 'encoder', 'network', 'topologically', 'identical', '13', 'convolutional', 'layer', 'vgg16', 'network', '1', 'role', 'decoder', 'network', 'map', 'low', 'resolution', 'encoder', 'feature', 'map', 'input', 'resolution', 'feature', 'map', 'pixel', 'wise', 'classiﬁcation', 'novelty', 'segnet', 'lie', 'manner', 'decoder', 'upsample', 'low', 'resolution', 'input', 'feature', 'map(s', 'speciﬁcally', 'decoder', 'use', 'pool', 'index', 'compute', 'max', 'pool', 'step', 'correspond', 'encoder', 'perform', 'non', 'linear', 'upsampling', 'eliminate', 'need', 'learn', 'upsample', 'upsampled', 'map', 'sparse', 'convolve', 'trainable', 'ﬁlter', 'produce', 'dense', 'feature', 'map', 'compare', 'propose', 'architecture', 'widely', 'adopt', 'fcn', '2', 'know', 'deeplab', 'largefov', '3', 'deconvnet', '4', 'architecture', 'comparison', 'reveal', 'memory', 'versus', 'accuracy', 'trade', 'involve', 'achieve', 'good', 'segmentation', 'performance', 'segnet', 'primarily', 'motivate', 'scene', 'understanding', 'application', 'design', 'efﬁcient', 'term', 'memory', 'computational', 'time', 'inference', 'signiﬁcantly', 'small', 'number', 'trainable', 'parameter', 'compete', 'architecture', 'train', 'end', 'end', 'stochastic', 'gradient', 'descent', 'perform', 'control', 'benchmark', 'segnet', 'architecture', 'road', 'scene', 'sun', 'rgb', 'd', 'indoor', 'scene', 'segmentation', 'task', 'quantitative', 'assessment', 'segnet', 'provide', 'good', 'performance', 'competitive', 'inference', 'time', 'efﬁcient', 'inference', 'memory', 'wise', 'compare', 'architecture', 'provide', 'caffe', 'implementation', 'segnet', 'web', 'demo', 'http://mi.eng.cam.ac.uk/projects/segnet/.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2_list_format = data2.split()\n",
        "\n",
        "print(data2_list_format)"
      ],
      "metadata": {
        "id": "P6rcdA43aznZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57463373-57b0-43a6-eded-99e6d44c5e4e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abstract', 'image', 'segmentation', 'key', 'task', 'computer', 'vision', 'image', 'processing', 'important', 'application', 'scene', 'understanding', 'medical', 'image', 'analysis', 'robotic', 'perception', 'video', 'surveillance', 'augmented', 'reality', 'image', 'compression', 'numerous', 'segmentation', 'algorithm', 'find', 'literature', 'backdrop', 'broad', 'success', 'deep', 'learning', 'dl', 'prompt', 'development', 'new', 'image', 'segmentation', 'approach', 'leverage', 'dl', 'model', 'provide', 'comprehensive', 'review', 'recent', 'literature', 'cover', 'spectrum', 'pioneering', 'effort', 'semantic', 'instance', 'segmentation', 'include', 'convolutional', 'pixel', 'labeling', 'network', 'encoder', 'decoder', 'architecture', 'multiscale', 'pyramid', 'base', 'approach', 'recurrent', 'network', 'visual', 'attention', 'model', 'generative', 'model', 'adversarial', 'setting', 'investigate', 'relationship', 'strength', 'challenge', 'dl', 'base', 'segmentation', 'model', 'examine', 'widely', 'dataset', 'compare', 'performance', 'discuss', 'promise', 'research', 'direction']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data3_list_format = data3.split()\n",
        "\n",
        "print(data3_list_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUNk2MUEXQ7N",
        "outputId": "1d4bbef2-e0a0-49c0-c653-dae49f95421e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abstract', 'state', 'art', 'object', 'detection', 'network', 'depend', 'region', 'proposal', 'algorithm', 'hypothesize', 'object', 'location', 'advance', 'like', 'sppnet', '1', 'fast', 'r', 'cnn', '2', 'reduce', 'run', 'time', 'detection', 'network', 'expose', 'region', 'proposal', 'computation', 'bottleneck', 'work', 'introduce', 'region', 'proposal', 'network', 'rpn', 'share', 'image', 'convolutional', 'feature', 'detection', 'network', 'enable', 'nearly', 'cost', 'free', 'region', 'proposal', 'rpn', 'fully', 'convolutional', 'network', 'simultaneously', 'predict', 'object', 'bound', 'objectness', 'score', 'position', 'rpn', 'train', 'end', 'end', 'generate', 'high', 'quality', 'region', 'proposal', 'fast', 'r', 'cnn', 'detection', 'merge', 'rpn', 'fast', 'r', 'cnn', 'single', 'network', 'share', 'convolutional', 'feature', 'recently', 'popular', 'terminology', 'neural', 'network', 'attention', 'mechanism', 'rpn', 'component', 'tell', 'uniﬁed', 'network', 'look', 'deep', 'vgg-16', 'model', '3', 'detection', 'system', 'frame', 'rate', '5fps', 'include', 'step', 'gpu', 'achieve', 'state', 'art', 'object', 'detection', 'accuracy', 'pascal', 'voc', '2007', '2012', 'ms', 'coco', 'dataset', '300', 'proposal', 'image', 'ilsvrc', 'coco', '2015', 'competition', 'fast', 'r', 'cnn', 'rpn', 'foundation', '1st', 'place', 'win', 'entry', 'track', 'code', 'publicly', 'available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer() converts a collection of text documents to a matrix of token counts.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "\n",
        "# Here we can take any data to feed in this model, Here we'll take \"data1_list_format\"\n",
        "# Tokenize and Build Vocabulary from the words that are present in \"data1_list_format\"\n",
        "vectorizer.fit(data1_list_format)"
      ],
      "metadata": {
        "id": "uHxrtWUBa10p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4029c292-e1a2-41be-eb7d-1c716f53e945"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the Data\n",
        "\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "id": "TMdyZjKgbF7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283f3d46-45d9-405d-e950-d29586e88d01"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'abstract': 1, 'present': 79, 'novel': 70, 'practical': 78, 'deep': 27, 'fully': 42, 'convolutional': 21, 'neural': 68, 'network': 67, 'architecture': 7, 'semantic': 94, 'pixel': 76, 'wise': 119, 'segmentation': 92, 'term': 103, 'segnet': 93, 'core': 23, 'trainable': 108, 'engine': 38, 'consist': 19, 'encoder': 35, 'correspond': 24, 'decoder': 25, 'follow': 41, 'classiﬁcation': 12, 'layer': 55, 'topologically': 105, 'identical': 46, '13': 0, 'vgg16': 116, 'role': 90, 'map': 61, 'low': 59, 'resolution': 86, 'feature': 40, 'input': 51, 'novelty': 71, 'lie': 57, 'manner': 60, 'upsample': 111, 'speciﬁcally': 98, 'use': 114, 'pool': 77, 'index': 48, 'compute': 18, 'max': 62, 'step': 99, 'perform': 74, 'non': 69, 'linear': 58, 'upsampling': 113, 'eliminate': 34, 'need': 66, 'learn': 56, 'upsampled': 112, 'sparse': 97, 'convolve': 22, 'ﬁlter': 120, 'produce': 81, 'dense': 30, 'compare': 13, 'propose': 83, 'widely': 118, 'adopt': 5, 'fcn': 39, 'know': 53, 'deeplab': 28, 'largefov': 54, 'deconvnet': 26, 'comparison': 14, 'reveal': 87, 'memory': 63, 'versus': 115, 'accuracy': 3, 'trade': 106, 'involve': 52, 'achieve': 4, 'good': 43, 'performance': 75, 'primarily': 80, 'motivate': 65, 'scene': 91, 'understanding': 110, 'application': 6, 'design': 32, 'efﬁcient': 33, 'computational': 17, 'time': 104, 'inference': 50, 'signiﬁcantly': 95, 'small': 96, 'number': 72, 'parameter': 73, 'compete': 15, 'train': 107, 'end': 36, 'stochastic': 100, 'gradient': 44, 'descent': 31, 'control': 20, 'benchmark': 9, 'road': 89, 'sun': 101, 'rgb': 88, 'indoor': 49, 'task': 102, 'quantitative': 85, 'assessment': 8, 'provide': 84, 'competitive': 16, 'caffe': 10, 'implementation': 47, 'web': 117, 'demo': 29, 'http': 45, 'mi': 64, 'eng': 37, 'cam': 11, 'ac': 2, 'uk': 109, 'projects': 82}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the Document\n",
        "\n",
        "# Now fit this sample text in the vector\n",
        "sample_text = [\"Hello, in this lab we will do segmentation and feature engineering using deep deep learning\"]\n",
        "\n",
        "new_vector = vectorizer.transform(sample_text)\n",
        "\n",
        "# Summarize the encoded vector\n",
        "print(new_vector.toarray())"
      ],
      "metadata": {
        "id": "bSTpHrjCbSgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f938fd3-646a-413c-92c8-1c2489ef9ca2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Observations:**\n",
        "> Here we can observe that the words \"deep\" and \"segmentation\" are present in our extracted title from document 1 and the sample text that we tested, hence it has values as 1 and else all are 0.\n",
        "<br>\n",
        "<br>\n",
        "> If there are common words between documents then the value will become 1 else if not common then value will become 0.\n",
        "<br>\n",
        "<br>\n",
        "> If same word is present multiple times then it shows the total count as well. In above cell we can see that \"image\" word is appeared 2 times in string as well as in the array.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7-v3uNsGaH49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, the above approach is not much useful because if one document talks about \"Image Segmentation\" and other talks about \"RNN's\" then it is not able to differentiate between both, hence it is not very helpful. so we make use of TF - IDF method, which is given below.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jifwztd-b4OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# >> TF - IDF "
      ],
      "metadata": {
        "id": "PZwisGJzpU3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer() converts a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "text_documents = [ data1,\n",
        "                   data2,\n",
        "                   data3 ]"
      ],
      "metadata": {
        "id": "3ZA9lKgLpn5H"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Transform\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "CRc-cdnZqBw0"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model by fitting the text_documents.\n",
        "vectorizer.fit(text_documents)"
      ],
      "metadata": {
        "id": "UwW9a6NnqGa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37947148-971e-4fd8-cd69-ecb817fa7225"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer()"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize - Printing the words from documents.\n",
        "\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "id": "1Tti5WSfqJHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d50488-4de6-42a9-954d-3dcdb77b69fc"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'abstract': 8, 'present': 167, 'novel': 148, 'practical': 165, 'deep': 61, 'fully': 92, 'convolutional': 52, 'neural': 145, 'network': 144, 'architecture': 19, 'semantic': 203, 'pixel': 160, 'wise': 249, 'segmentation': 201, 'term': 224, 'segnet': 202, 'core': 54, 'trainable': 231, 'engine': 80, 'consist': 50, 'encoder': 77, 'correspond': 55, 'decoder': 59, 'follow': 88, 'classiﬁcation': 34, 'layer': 119, 'topologically': 227, 'identical': 101, '13': 0, 'vgg16': 241, 'role': 196, 'map': 131, 'low': 129, 'resolution': 190, 'feature': 86, 'input': 110, 'novelty': 149, 'lie': 123, 'manner': 130, 'upsample': 235, 'speciﬁcally': 211, 'use': 238, 'pool': 162, 'index': 107, 'compute': 48, 'max': 132, 'step': 215, 'perform': 157, 'non': 147, 'linear': 125, 'upsampling': 237, 'eliminate': 75, 'need': 143, 'learn': 120, 'upsampled': 236, 'sparse': 210, 'convolve': 53, 'ﬁlter': 251, 'produce': 170, 'dense': 64, 'compare': 38, 'propose': 175, 'widely': 247, 'adopt': 12, 'fcn': 85, 'know': 116, 'deeplab': 62, 'largefov': 118, 'deconvnet': 60, 'comparison': 39, 'reveal': 191, 'memory': 135, 'versus': 239, 'accuracy': 10, 'trade': 229, 'involve': 114, 'achieve': 11, 'good': 95, 'performance': 158, 'primarily': 168, 'motivate': 139, 'scene': 199, 'understanding': 233, 'application': 17, 'design': 67, 'efﬁcient': 74, 'computational': 47, 'time': 226, 'inference': 109, 'signiﬁcantly': 206, 'small': 209, 'number': 150, 'parameter': 154, 'compete': 40, 'train': 230, 'end': 78, 'stochastic': 216, 'gradient': 97, 'descent': 66, 'control': 51, 'benchmark': 27, 'road': 194, 'sun': 219, 'rgb': 193, 'indoor': 108, 'task': 222, 'quantitative': 180, 'assessment': 21, 'provide': 176, 'competitive': 42, 'caffe': 31, 'implementation': 104, 'web': 246, 'demo': 63, 'http': 99, 'mi': 137, 'eng': 79, 'cam': 32, 'ac': 9, 'uk': 232, 'projects': 171, 'image': 103, 'key': 115, 'computer': 49, 'vision': 243, 'processing': 169, 'important': 105, 'medical': 134, 'analysis': 16, 'robotic': 195, 'perception': 156, 'video': 242, 'surveillance': 220, 'augmented': 23, 'reality': 182, 'compression': 45, 'numerous': 151, 'algorithm': 15, 'find': 87, 'literature': 126, 'backdrop': 25, 'broad': 30, 'success': 218, 'learning': 121, 'dl': 72, 'prompt': 173, 'development': 69, 'new': 146, 'approach': 18, 'leverage': 122, 'model': 138, 'comprehensive': 44, 'review': 192, 'recent': 183, 'cover': 57, 'spectrum': 212, 'pioneering': 159, 'effort': 73, 'instance': 111, 'include': 106, 'labeling': 117, 'multiscale': 141, 'pyramid': 178, 'base': 26, 'recurrent': 185, 'visual': 244, 'attention': 22, 'generative': 94, 'adversarial': 14, 'setting': 204, 'investigate': 113, 'relationship': 188, 'strength': 217, 'challenge': 33, 'examine': 82, 'dataset': 58, 'discuss': 71, 'promise': 172, 'research': 189, 'direction': 70, 'state': 214, 'art': 20, 'object': 152, 'detection': 68, 'depend': 65, 'region': 187, 'proposal': 174, 'hypothesize': 100, 'location': 127, 'advance': 13, 'like': 124, 'sppnet': 213, 'fast': 84, 'cnn': 35, 'reduce': 186, 'run': 198, 'expose': 83, 'computation': 46, 'bottleneck': 28, 'work': 250, 'introduce': 112, 'rpn': 197, 'share': 205, 'enable': 76, 'nearly': 142, 'cost': 56, 'free': 91, 'simultaneously': 207, 'predict': 166, 'bound': 29, 'objectness': 153, 'score': 200, 'position': 164, 'generate': 93, 'high': 98, 'quality': 179, 'merge': 136, 'single': 208, 'recently': 184, 'popular': 163, 'terminology': 225, 'mechanism': 133, 'component': 43, 'tell': 223, 'uniﬁed': 234, 'look': 128, 'vgg': 240, '16': 1, 'system': 221, 'frame': 90, 'rate': 181, '5fps': 7, 'gpu': 96, 'pascal': 155, 'voc': 245, '2007': 3, '2012': 4, 'ms': 140, 'coco': 36, '300': 6, 'ilsvrc': 102, '2015': 5, 'competition': 41, 'foundation': 89, '1st': 2, 'place': 161, 'win': 248, 'entry': 81, 'track': 228, 'code': 37, 'publicly': 177, 'available': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observations:**\n",
        "> The words that are appearing most frequently has the lowest value as they can't give us more details about a particular document.\n",
        "\n",
        ">The words that are Unique are considered as that helps us to find the Importance of a word in any particular document."
      ],
      "metadata": {
        "id": "YN6hvHwjlaSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The IDF Values\n",
        "print(vectorizer.idf_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD7jMAiNrmj8",
        "outputId": "d511aa6e-9f6a-4e20-dcb5-5e4fd83d4c8b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.         1.69314718 1.28768207 1.28768207\n",
            " 1.69314718 1.69314718 1.69314718 1.28768207 1.69314718 1.28768207\n",
            " 1.69314718 1.28768207 1.69314718 1.69314718 1.28768207 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.         1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207 1.28768207\n",
            " 1.69314718 1.         1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207\n",
            " 1.28768207 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.28768207 1.69314718 1.69314718 1.28768207 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.28768207 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.         1.28768207 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.28768207 1.69314718 1.28768207 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.28768207 1.69314718 1.28768207 1.69314718 1.28768207\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.28768207 1.69314718 1.69314718 1.69314718 1.28768207 1.69314718\n",
            " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.28768207\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718 1.69314718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trying the same above steps but using Sample Random Raw Data.**"
      ],
      "metadata": {
        "id": "LMWuh6mUmsK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text_documents = [ \"Data is booming these days\",\n",
        "                   \"Every individual is shifting from their domain to data science\",\n",
        "                   \"User Data is the most important thing in today's world\"\n",
        "                   \"Data is the new oil\" ]"
      ],
      "metadata": {
        "id": "uHLWLnvJu6Mg"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "8oFpimOmm2AT"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.fit(sample_text_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB0tDpgcm-wx",
        "outputId": "7867a641-30cf-4ac2-f30d-9274ee4c1a86"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer()"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dm2TMPIvHf7",
        "outputId": "d4b6b582-1ae0-4918-fc68-d82b044dfe1f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': 1, 'is': 9, 'booming': 0, 'these': 17, 'days': 2, 'every': 4, 'individual': 8, 'shifting': 14, 'from': 5, 'their': 16, 'domain': 3, 'to': 19, 'science': 13, 'user': 21, 'the': 15, 'most': 10, 'important': 6, 'thing': 18, 'in': 7, 'today': 20, 'worlddata': 22, 'new': 11, 'oil': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.idf_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n70tzFaHvOTc",
        "outputId": "f14a88fa-6c73-4e66-e1ce-c2c8e266a2a3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.69314718 1.         1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.         1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observations:**\n",
        "\n",
        "> From the above cell, we can see that At 2nd Index, the IDF value is 1. which points to \"data\", which is comparatively less than other values.\n",
        "\n",
        "> Hence, \"Data\" word is repeating right, so we have suppressed it and lowered its value so that we get to know the importance of other unique words."
      ],
      "metadata": {
        "id": "cF-nA7J6nRr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passing one document into our Model and obtaining the results.**"
      ],
      "metadata": {
        "id": "7l0KA54GoXR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If we take only one document\n",
        "single_document = sample_text_documents[1]\n",
        "\n",
        "\n",
        "# Fitting the above built model (vectorizer)\n",
        "vector = vectorizer.transform([single_document])"
      ],
      "metadata": {
        "id": "DA7EreQevUZc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarizing the Encoded Vector\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGPY5l5avuv0",
        "outputId": "443b285c-4596-4648-b584-9b6da67385c4"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.20026461 0.         0.33907746 0.33907746 0.33907746\n",
            "  0.         0.         0.33907746 0.20026461 0.         0.\n",
            "  0.         0.33907746 0.33907746 0.         0.33907746 0.\n",
            "  0.         0.33907746 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        ">Hence Feature Engineering and Segmentation on text data was performed successfully.\n",
        "<br>\n",
        "\n",
        ">Using Vector Values, we can understand the Importance of a word present in the document.\n",
        "<br>\n",
        "\n",
        ">TF - IDF method is mostly used when we want to deal with text data as it has advantages as compared to the basic CountVectorizer.\n"
      ],
      "metadata": {
        "id": "9dKiuETtokWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####------------------------------------------------------------------\n",
        "\n",
        "####**Name:** Sabyasachi Singh\n",
        "\n",
        "<br>\n",
        "\n",
        "####**PRN:** 20190802051\n",
        "\n",
        "#----------------------------------------------"
      ],
      "metadata": {
        "id": "heCRPVeWgKA3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}